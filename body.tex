\section{Introduction}

Recently the Linux container clusters draw much attention because they are lightweight, portable and repeatable.
Container clusters are generally more lightweight than Virtual Machine clusters, 
because the containers share the kernel with host OS, still having separate execution environment. 
They are generally portable because the process execution environments are archived into the tar files.
Whenever one attempts to run a container, the exact same file systems are restored from the archives, 
even on totally different data centers. 
This means a container cluster can provide the repeatable and portable execution environment.
For WEB services, the Linux containers are attractive because of the same reasons. 
Furthermore, it is expected that WEB services consisting of container clusters are 
capable of being easily migrated for Disaster Recovery purposes or for other business requirements.

The Kubernetes\cite{K8s2017} is one of the popular container cluster management systems, 
which enable easy deployment of container clusters.
By using the Kubernetes as middle-ware, we can easily deploy a WEB service on different 
cloud providers or on on-premise data centers, without changing the container cluster configuration to the base environment. 
This allows a user to easily migrate a WEB service consisting of a container cluster even to the different side of earth. 
The typical scenario is that the user starts the container cluster in a new location and changes the network configuration 
so that the traffic is routed to the new location. Then, he can stop the old container cluster eventually.

However, this scenario only works when the user migrates a container cluster among major cloud providers including GCP, AWS and Azure.
The reason is that the Kubernetes does not include a load balancer, 
and it is heavily dependent on external load balancers offered by the cloud providers hosting the Kubernetes on the fly through its API. 
In this scenario, the external load balancer will distribute incoming traffic to servers that host containers.
The traffic is then distributed again to destination containers using iptables DNAT\cite{MartinA.Brown2017,Marmol2015} 
rules with a round-robin manner. 
The problem happens in the environment with a load balancer that is not supported by the Kubernetes, 
e.g. a bare metal load balancer in an on-premise data center. In this case, the user needs to manually configure 
the static route setting for inbound traffic with an ad-hoc manner. 
Thus, migrating container clusters among those environments will always be a burden.

In order to solve this problem by eliminating the dependency on external load balancer in the Kubernetes, 
we propose a containerized software load balancer. 
The proposed load balancer runs in container clusters operated by the Kubernetes. 
It enables the user to easily deploy a WEB service on different environment without modification. 
We implemented the proposed load balancer by creating containers for Linux kernel's ipvs\cite{Zhang2000} 
layer 4 load balancer using the existing ingress\cite{K8sIngress2017} framework of the Kubernetes. 
We also investigate the feasibility of the proposed load balancer by comparing the performance 
with the native iptables DNAT load balancer and the Nginx layer 7 load balancer. 
The results indicated that the proposed load balancer could improve the portability of container clusters 
without performance degradation compared with the existing load balancer. 
The performance of the proposed load balancer may be affected by the network configurations of overlay network 
and distributed packet filtering. 
We also evaluate how the network configurations affects the performance and discusses 
the best setting that derive the best performance.

The contribution of this paper are followings: 
1) We propose a portable software load balancer, 
which is runnable on any cloud provider or on on-premise data center, 
in a container cluster.
2) We discuss feasibility of the proposed load balancer by comparing the performance.
3) We also discuss about usable overlay network configurations and clarify importance of a technique 
to draw the best performance from such load balancers.  

The rest of the paper is organized as follows.
The section \ref{Related Work} highlights related work specifically those deal with container cluster migration, 
software load balancer containerization or a load balancer related tools in the context of the container technology. 
The section \ref{Load balancers in Kubernetes cluster} will explain the problem of the existing architecture and propose our solution.
In the section \ref{Performance Measurement}, experimental conditions and the important parameters 
we considered in the experiment will be described in detail.
Then we will show the result of experiment and discuss the obtained performance characteristics in the section~\ref{Result and Discussion},  
which is followed by the conclusion in the section~\ref{Conclusions}.

\section{Related Work}\label{Related Work}

This section highlights related work especially those deal with container cluster migration, 
software load balancer containerization or load balancer tools in the context of the container technology.

\paragraph{\bf Container cluster migration:}

Developers of the Kubernetes are trying to add federation\cite{K8sFederation2017} capability 
where multiple Kubernetes clusters are deployed on multiple cloud providers or on-premise data centers 
and are managed via the federation apiserver server. However, how each of Kubernetes clusters is run on different type of cloud providers
or in on-premise data centers, especially when the load balancers of such environments are not supported by the Kubernetes, 
seems out of the scope of that project. 
The main scope of this paper is to make the Kubernetes usable on environments where there is no supported load balancers
by providing a containerized load balancer.

\paragraph{\bf Software load balancer containerization:}
As far as the containerization of a load balancers are concerned, there are following related works:
Nginx-ingress\cite{Pleshakov2016,NginxInc2016} utilizes the ingress\cite{K8sIngress2017} capability of the Kubernetes, 
and implements containerized Nginx proxy as a load balancer. The Nginx itself is famous as a high performance web server program,
which also has the functionality of layer 7 load balancer. The Nginx is capable of handling TLS encryption and also capable of  
URI based switching. However the flip side of these is that it is much slower than layer 4 switching.
The upper bound of the performance of the Nginx as a load balancer will not be much better than 
that as a single web server serving lightweight web pages. 
Therefore it is meaningless to use the Nginx as a load balancer in such cases.
The kube-keepalived-vip\cite{Prashanth2016} project is trying to use the Linux kernel's load balancer capability, 
called ipvs\cite{Zhang2000}, by containerizing the keepalived\cite{ACassen2016} and ipvs. 
The keepalived container is using the daemonsets\cite{K8sDaemonsets2017} framework of the Kubernetes. 
In this case the container is run on every node.
This work is very similar to our proposal in the sense that it uses keepalived and ipvs in container. 
However it does not provide per container cluster load balancer which is portable among different cloud providers.   
Our proposal is to provide a load balancer that is configurable and deploy-able at user's will,
 as a part of container cluster itself not as a port of the Kubernetes cluster infrastructure.  
The swarm mode of the docker\cite{DockerCoreEngineering2016,DockerInc2017} also uses ipvs for internal load balancing,
however it is also considered as built-in load balancer functionality of the docker, 
and is different from our proposal.

\paragraph{\bf Load balancer tools in the context of container:}
There are several other projects where an effort to utilize ipvs in the context of container environment.
The gorb\cite{Sibiryov2015} and the clusterf\cite{Aaltodoc:http://urn.fi/URN:NBN:fi:aalto-201611025433} are daemons 
which setups ipvs rules in the kernel inside docker container. They utilize information about running container stored in key-value storages
like the etcd\cite{CoreOSEtcd} and the consul\cite{HashiCorpConsul}. 
These has the similar functionality to a combination of keepalived and the Kubernetes ingress controller in our proposal.
These may be usable in our environment in the future.

\section{Load balancers in Kubernetes cluster}\label{Load balancers in Kubernetes cluster}

\subsection{Problems of Conventional Architecture}

\begin{figure}
\includegraphics[width=\columnwidth]{Figs/K8sConventional}
\caption{Conventional Architecture of A Kubernetes cluster.}
\label{fig:K8sConventional}
\end{figure}

The Kubernetes container management system has an issue when it is used in outside of recommended cloud providers e.g. GCP or AWS.
Figure~\ref{fig:K8sConventional} shows an exemplified Kubernetes cluster.
The Kubernetes cluster typically consists of master servers and node servers.
On the master servers daemons that control the Kubernetes cluster are typically deployed. 
These daemons include, apiserver, scheduler, controller-manager and etcd. 
On the node servers, the kubelet daemon will run pods, depending the PodSpec information obtained from the apiserver on the master servers.
A pod is a group of containers sharing net name space and cgroups, 
and it is the basic execution unit in the Kubernetes cluster.

When a service is created, the masters will send out request to cloud providers API endpoints to set up external load balancers.
The proxy daemon on the node servers will also setup iptables DNAT\cite{MartinA.Brown2017} rules, 
which distributes the inbound traffic to the existing pods.
The traffic from the internet will be distributed by the external load balancer to node servers evenly, 
then it will be distributed again by the DNAT rules on the node serves to designated pods. 
The returning packets will follow the exact same route as the incoming ones.

The problems of this architecture are the followings: 
1) Having external load balancers whose APIs are supported by the Kubernetes daemons is prerequisite. 
There are many cloud providers where the API of load balancers is not supported by Kubernetes. 
There are many load balancers which dose not even have APIs that is usable by Kubernetes, 
especially bare metal load balancers for on-premise data centers.  
In such cases, one could manually setup the routing table on the gateway so that the traffic will be routed to destination node servers.
Then the traffic will be distributed by the DNAT rules on the node to designated pods.
However, this approach requires complicated network configuration and significantly degrades the portability of container clusters.
2) Distributing the traffic twice, firstly on the external load balancers and secondary on each node, 
will complicate the administration of packet routing. 
Imagine the situation, where the DNAT table on one of the node servers malfunctions.
In such case, one will only observe occasional timeouts. It is very difficult to find out which node server malfunctions.   

So in short, 1) the Kubernetes can be used only in limited environments where the external load balancers are supported, 
2) the route incoming traffic follows are very complex.

In order to address these problems, we propose a containerized software load balancer, 
which is deploy-able on any environment even if there are no external load balancers.

\subsection{Proposed Architecture}

\begin{figure}
\includegraphics[width=\columnwidth]{Figs/K8sProposed}
\caption{A Kubernetes cluster with proposed load balancer.}
\label{fig:K8sProposed}
\end{figure}

Figure~\ref{fig:K8sProposed} shows the architecture of the proposed load balancer for the Kubernetes cluster. 
The proposed load balancer has following characteristics:
1) Each load balancer itself is run as a pod by the Kubernetes. 
2) Configuration of the load balancers are dynamically updated depending on the information of running pods.
%%3) There exist multiple load balancers for redundancy. 
The proposed load balancer solved the the problems of conventional architecture as follows:
The load balancer can run on any environment, even on the environment without external load balancers supported by the Kubernetes including on-premise data centers, since the load balancer itself is containerized. The incoming traffic is directly distributed to destination pods by the proposed load balancer. It makes the administration, e.g. finding malfunctions, easier.

We designed the proposed load balancer using three components, ipvs, keepalived and ingress controller. The components are implemented as docker images.
The ipvs is a layer 4 load balancer, which is included in the Linux kernel 2.6.0 released in 2003 or later, to distribute incoming TCP traffic to server programs. For example, ipvs distributes incoming HTTP traffic with one destination IP address to multiple HTTP servers (e.g. Apache HTTP or nginx) running on multiple servers to improve the performance of WEB service.
The keepalived is a management program to perform health checking of real servers
and manage the ipvs balancing rules in the kernel accordingly.
It is often used with the ipvs to provide the ease of use.
The keepalived also provides VRRP\cite{hinden2004virtual} redundancy, but that is beyond the scope of this paper.
The ingress controller is a daemon to periodically monitor the pod information on the master, 
and does some actions upon change of those information.
The Kubernetes provides the framework as the Golang package to implement such controller. 
We implemented a controller program which will feed the pod state change to the keepalived 
using this framework. 

%These are the main idea of our proposal, the following section explains the implementation in detail.

\subsection{Implementation}\label{Implementation}

\begin{figure}
\includegraphics[width=\columnwidth]{Figs/ipvs-ingress-schem}
\caption{Implementation}
\label{fig:ipvs-ingress-schem}
\end{figure}

The proposed load balancer needs to dynamically reconfigure the ipvs balancing rules whenever pods are created/deleted. 
Figure~\ref{fig:ipvs-ingress-schem} shows a schematic diagram to the dynamic reconfiguration of the rules.
Two daemons programs, controller and keepalived, run in the container illustrated in the right part of the figure.
The keepalived manages Linux kernel's ipvs rules depending on the configuration file, ipvs.conf.
The keepalived is also capable of health-checking of real-server, 
which is represented as a combination of IP address and port number of the target pods. 
If the health-check to a real server fails, the keepalived will remove that real server from the ipvs rules.

The controller monitors information of running pods of a service in the Kubernetes cluster by consulting the apiserver running on the master server.
Whenever pods are created or deleted, it will automatically regenerate appropriate ipvs.conf and issue SIGHUP to the keepalived.
Then the keepalived will reload the ipvs.conf and modify the kernel's ipvs rules accordingly.  
The actual controller\cite{ktaka_ccmp_2017_826894} is implemented using ingress controller\cite{K8sIngress2017} framework of the Kubernetes. 
By importing existing Golang package, \enquote{k8s.io/ingress /core/pkg/ingress}, we can simplify the implementation, e.g. 
120 lines of code.  

%In this way, the ipvs's balancing rules inside Linux kernel are maintained so that it can distribute the incoming traffic only to the living pods.

Configurations for capabilities are needed in the implementation: adding the CAP\_SYS\_MODULE capability 
to the container to allow the kernel to load required kernel modules inside a container, 
and adding CAP\_NET\_ADMIN capability to the container to allow the keepalived to manipulate kernel’s ipvs rules. 
For the former case, we also need to mount the \enquote{/lib/module} of the node's file system on the container's file system.
There is an explanation in \cite{mp2016enhancing} about some of the capabilities 
that is usually dropped inside docker containers.


\begin{figure}
\begin{minipage}{0.7\columnwidth}
\begin{verbatim}
virtual_server fwmark 1 {
  delay_loop 5
  lb_algo lc
  lb_kind NAT
  protocol TCP
  real_server 172.16.21.2 80 {
    uthreshold 20000
    TCP_CHECK {
      connect_timeout 5
      connect_port 80
    }
  }
  real_server 172.16.80.2 80 {
    uthreshold 20000
    TCP_CHECK {
      connect_timeout 5
      connect_port 80
    }
  }
}
\end{verbatim}
\end{minipage}
\caption{An example of ipvs.conf}
\label{fig:ipvs.conf}
\end{figure}

\begin{figure}
\begin{minipage}{\columnwidth}
\small
\begin{verbatim}
# kubectl exec -it ipvs-controller-4117154712-kv633 -- ipvsadm -L
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port Forward Weight ActiveConn InActConn
FWM  1 lc
  -> 172.16.21.2:80      Masq    1      0          0         
  -> 172.16.80.2:80      Masq    1      0          0
\end{verbatim}
\end{minipage}
\caption{An example of ipvs balancing rules}
\label{fig:ipvs rule}
\end{figure}


Figure~\ref{fig:ipvs.conf} and Figure~\ref{fig:ipvs rule} show an example of the ipvs.conf file 
generated by the controller and the corresponding ipvs's load balancing rules, respectively.
We can see that the packet with {\tt fwmark=1}\cite{BertHubert2002} is distributed 
to the {\tt 172.16.21.2:80} and {\tt 172.16.80.2:80} 
using the masquerade mode(Masq) and 
the least connection(lc)\cite{Zhang2000} balancing algorithm. 

\section{Performance Measurement}\label{Performance Measurement}


We discuss the feasibility of the proposed load balancer by comparing the performance with the existing iptables DNAT and the Nginx based load balancer. 
We conducted the performance measurement using the benchmark program called, wrk\cite{Glozer2016}.
%We also measured performance of iptables's DNAT load balancing functionality and Nginx layer 7 load balancer for comparison.

We also investigated the performance under two network configurations: 
First one is an overlay network setting\cite{Sill2016,Marmol2015} that is often used to build the Kubernetes cluster. 
The flannel\cite{CoreOSFlannel} is one of popular overlay network technologies. 
We compared the performances of three backends settings\cite{CoreOSFlannelBackend}, 
{\it i.e.} operating modes of the flannel, to find the best setting.
The other one is the setting for distributed packet filtering.
It is known that distributing interrupt handring from the NIC and the following IP protocol processing 
among multiple cores impact the network performance.
In order to derive the best performance from a load balancer, 
we also investigated how this setting would affect the performance of load balancers.

%The following subsections explain these further in detail. 

\subsection{Benchmark method}

\begin{figure}
\includegraphics[width=\columnwidth]{Figs/benchmark-schem}
\caption{Benchmark setup}
\label{fig:benchmark-schem}
\end{figure}

%Using the wrk, we measured performance of the load balancers.
Figure~\ref{fig:benchmark-schem} illustrates a schematic diagram of the experimental setting.
Nginx web servers, which return IP address of the pods hosting the web servers, 
running on pods are deployed on multiple nodes in the Kubernetes cluster. 
We set up the ipvs, iptables DNAT and Nginx load balancers on one of the node servers. 

We measured the throughput, request/sec, of the WEB service running on the Kubernetes cluster as follows:
The HTTP GET requests are sent out by the wrk on the client machine toward the node servers.
The destination IP address and port number is chosen 
depending on the type of the load balancer on which the measurement is performed.
The load balancer on the node server then distribute the requests to the pods.
Each pod will return HTTP responses to the load balancer and the load balancer will then 
return the response to the client.
The number of responses received by the wrk on the client is counted, 
then the performance in terms of the request/sec is obtained. 

Figure~\ref{fig:benchmark example} shows an example of the command-line for the wrk and the corresponding output.
The command-line in Figure~\ref{fig:benchmark example} will generate 40 threads of wrk program 
and lets those threads send out in total of 800 concurrent HTTP requests during 30 seconds.
The example output shows information including per thread statistics, error counts, Request/sec and Transfer/sec[MByte/sec].
%We considered the ¥enquote{Request/sec} to be the performance, and compared them among several conditions.

Figure~\ref{fig:Hardware and software configuration} shows hardware and software configuration used in our experiments.
We configured the Nginx HTTP server to response a small HTTP content, 
the IP address of the pod, to make relatively severe condition for load balancers. 
The size of the characters of an IP address is only up to 15 bytes.
If we choose the size of the HTTP response so that most of the IP packet result in MTU,
The performance is limited by the bandwidth of the Ethernet. 
However, the performance is limited by the load balancer in the experimental setting with small HTTP resonses.

Each of the hardwares we used had a network card(NIC) with 4 rx-queues.
It had 8 physical CPU cores, which will become 16 logical core when the Hyper Threading is enabled.

\begin{figure}
\begin{minipage}{\columnwidth}
\small
\begin{Verbatim}[commandchars=\\\{\}]
\underline{\textbf{Command line:}}
 wrk -c800 -t40 -d30s http://172.16.72.2:8888/
-c: concurrency, -t: # of thread, -d: duration

\underline{\textbf{Result sample:}}
 Running 30s test @ http://10.254.0.10:81/
  40 threads and 800 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency    15.82ms   41.45ms   1.90s    91.90%
    Req/Sec     4.14k   342.26     6.45k    69.24%
  4958000 requests in 30.10s, 1.14GB read
  Socket errors: connect 0, read 0, write 0, timeout 1
Requests/sec: 164717.63
Transfer/sec:     38.86MB
\end{Verbatim}
\end{minipage}
\caption{An example output of benchmark by wrk}
\label{fig:benchmark example}
\end{figure}

\begin{figure}
\begin{minipage}{0.9\columnwidth}
\small
\begin{Verbatim}[commandchars=\\\{\}]
\underline{\textbf{Physical Machine Specification:}}
  CPU: Intel(R) Xeon(R) CPU E5-2450 0 @ 2.10GHz
  # of Physical Cores: 8
  Hyper Threading: enabled
  Memory: 32GB
  NIC: Broadcom BCM5720 Gigabit Ethernet PCIe

\underline{\textbf{Number of Physical Machines:}}
  Master: 1
  Node: 7
  Client: 1

\underline{\textbf{Node Software version:}}
  OS: Debian 8.7 (Jessie)
  Kernel: \footnotesize{3.16.0-4-amd64 #1 SMP Debian 3.16.39-1 (2016-12-30)}
  Kubernetes v1.5.2
  flannel v0.7.0
  etcd version: 3.0.15

\underline{\textbf{Load balancer Software version:}}
  Keepalived: v1.3.2 (12/03,2016)
  Nginx: nginx/1.11.1

\underline{\textbf{Worker Pod Software version:}}
  nginx : nginx/1.13.0 
\end{Verbatim}
\end{minipage}
\caption{Hardware and software configuration}
\label{fig:Hardware and software configuration}
\end{figure}


\subsection{Overlay network}

%When building a Kubernetes cluster, one has a choice in usable overlay networks.
We used the flannel to build the Kubernetes cluster used in our experiment. 
The flannel has three types of backend （{\it i.e.} operating mode）, called host-gw, vxlan and udp\cite{CoreOSFlannelBackend}.

In the host-gw mode, the flanneld on a node server simply configures the routing table 
based on the IP address assignment information of the overlay network stored in the etcd. 
When a pod on a node sends out an IP packet to pods on the different node server, 
the former node server consults the routing table and learn that the IP packet should be sent out to the latter node.
Then the former node forms Ethernet frames having the destination MAC address of the latter node without changing the IP header and send out the IP packet.

In the case of the vxlan mode, the flanneld creates the Linux kernel's vxlan device, flannel.1. 
The flanneld also configures the routing table appropriately based on the information stored in the etcd.
When pods on different nodes need to communicate, the packet is routed to flannel.1.
The vxlan functionality of the Linux kernel searches the MAC address of flannel.1 device on the destination node server and  
then forms an Ethernet frame toward the MAC address.
The vxlan then encapsulates the Ethernet frame in a UDP/IP packet with the vxlan header and send out the IP packet.

In the case of udp mode, the flanneld creates the tun device, flannel0, and configures the routing table.
The flannel0 device is connected to flanneld daemon itself.
The IP packet routed to flannel0 is encapsulated by the flanneld, and sent out 
to the appropriate node server. 
The encapsulation is done for IP packets.

Figure~\ref{fig:flannel-packet-diagram} shows the formats of Ethernet frames in the flannel overlay network with three backends. 
The MTU sizes in the backends, assuming the MTU size without encapsulation is 1500byte, are also presented.
Since packets are not encapsulated in the host-gw mode, the MTU size remains 1500bytes.
Additional 50bytes of header is used in the vxlan mode, resulting in MTU size of 1450bytes.
In the case of the udp mode, only 28bytes of header is used for encapsulation, which results in MTU size of 1472bytes.

\begin{figure}
\includegraphics[width=\columnwidth]{Figs/flannel-packet-diagram}
\caption{frame diagram}
\label{fig:flannel-packet-diagram}
\end{figure}

The performance of the load balancers might be influenced by the overhead of encapsulation. 
Thus, the host-gw mode, where there is no overhead due to encapsulation, 
results in the best performances as shown in Section~\ref{Result and Discussion}.
However, the host-gw mode has a significant drawback that it does not work correctly in some cloud platforms. 
In other words, a gateway cannot detect the proper destination of a packet and drop the packet, 
since the host-gw mode simply sends out a packet without encapsulation.

\begin{table}
  \begin{tabular}{lccc}
    \toprule
    flannel backend & On-premise & GCP & AWS \\
    \midrule
    host-gw & OK & NG & (OK) \\
    vxlan & OK & OK & OK \\
    udp & OK & OK & OK \\
    \bottomrule
\end{tabular}
  \caption{Viable flannel backends}
  \label{tab:Viable flannel backends}
\end{table}

%This is known to happen in the case of cloud providers.  
We investigated which backend of the flannel is usable on AWS, GCP and on-premise data centers.
The results are summarized in Table~\ref{tab:Viable flannel backends}
%
In the case of GCP, an IP address of {\tt /32} is assigned to every VM host.
Every communication between VMs goes through GCP's gateway.
As for AWS, VMs within the same subnet communicate directly, while
a communication is via the AWS's gateway if the VMs reside in different subnets.
The gateways do not have knowledge of the flannel overlay network; thus, it prohibits the use of the host-gw mode of the flannel on the environment.  

%In the experiment, we compared the performance of load balancers when different backend of the flannel is used. 

\subsection{Distributed packet handling}

Recently the performance of a CPU is improved significantly due to the development of multi-core CPUs.
One of the top of the line server processors from Intel even has up to 28 cores in a single CPU.
In order to enjoy the benefit of such many cores in communication performance, 
it is known to be necessary to distribute the handling of interrupts from the NIC and the IP protocol processing
to the available physical cores.
The Receive Side Scaling (rss)\cite{TomHerbert} is the technology 
to distribute handling of the interrupt from NIC queues to multiple CPU cores.
Then the Receive Packet Steering (rps)\cite{TomHerbert} distributes the IP protocol processing 
to multiple CPU cores by issuing inter core software interrupt.

The performance of a load balancer may be affected by these technologies.
Therefore, we compared how the performance of a load balancer changes depending on the rss and rps settings.
The following shows how the rss and rps are enabled and disabled in our experiment. 
The Network Interface Card (NIC) used in the experiment is Broadcom BCM5720, which has four rx-queues and one tx-queue.
Figure~\ref{fig:rx-queue} shows the IRQ number assignment to those NIC queues.

\begin{figure}
\begin{minipage}{0.8\columnwidth}
\small
\begin{verbatim}

81: eth0-tx-0
82: eth0-rx-1
83: eth0-rx-2
84: eth0-rx-3
85: eth0-rx-4
# obtained from /proc/interrupts 

\end{verbatim}
\end{minipage}
\caption{RX/TX queues of the hardware}
\label{fig:rx-queue}
\end{figure}

When packets arrive, they are distributed to these rx-queues depending on the flow each packet belongs to.
Each receive queue has a separate IRQ associated with it. The NIC triggers
this to notify a CPU when new packets arrive on the given queue.
Then, the notified CPU handles the interrupt and performs the protocol processing. 
According to the \cite{TomHerbert}, the CPU cores allowed to be notified is controlled by setting 
hexadecimal value corresponding to the bit maps indicating the allowed CPU cores in \enquote{/proc/irq/\$irq\_number /smp\_affinity}.
%
For example, in order to route the interrupt for eth0-rx-1 to CPU0, 
we should set \enquote{/proc/irq/82/smp\_affinity} 
to binary number {\tt 0001}, which is 1 in hexadecimal value.
And in order to route the interrupt for eth0-rx-2 to CPU1, we 
should set \enquote{/proc/irq/83/smp\_affinity} 
to binary number {\tt 0010}, which is 2 in hexadecimal value.

We refer the setting to distribute interrupts from four rx-queues to CPU0, CPU1, CPU2 and CPU3 as {\tt rss = on}. 
It is configured as the following setting: 

\begin{center}
\begin{minipage}{0.8\columnwidth}
\begin{Verbatim}[commandchars=\\\{\}]

\underline{\textbf{rss=on}}
echo 1 > /proc/irq/82/smp_affinity
echo 2 > /proc/irq/83/smp_affinity
echo 4 > /proc/irq/84/smp_affinity
echo 8 > /proc/irq/85/smp_affinity

\end{Verbatim}
\end{minipage}
\end{center}

On the other hand, the {\tt rss = off} means that interrupt from any of rx-queue is routed to CPU0. 
It is configured as the following setting:

\begin{center}
\begin{minipage}{0.8\columnwidth}
\begin{Verbatim}[commandchars=\\\{\}]

\underline{\textbf{rss=off}}
echo 1 > /proc/irq/82/smp_affinity
echo 1 > /proc/irq/83/smp_affinity
echo 1 > /proc/irq/84/smp_affinity
echo 1 > /proc/irq/85/smp_affinity

\end{Verbatim}
\end{minipage}
\end{center}

%In this case, interrupt from any of rx-queue is routed to CPU0.

The rps distributes protocol processing of the packet by placing the packet
on the desired CPU's backlog queue and wakes up the CPU using inter-processor interrupts.
We have used the following settings to enable the rps:

\begin{center}
\begin{minipage}{0.8\columnwidth}
\begin{Verbatim}[commandchars=\\\{\}]

\underline{\textbf{rps=on}}
echo fefe > /sys/class/net/eth0/queues/rx-0/rps_cpus
echo fefe > /sys/class/net/eth0/queues/rx-1/rps_cpus
echo fefe > /sys/class/net/eth0/queues/rx-2/rps_cpus
echo fefe > /sys/class/net/eth0/queues/rx-3/rps_cpus

\end{Verbatim}
\end{minipage}
\end{center}

The hexadecimal value \enquote{fefe} represented as \enquote{1111 1110 1111 1110} in binary, 
thus this setting will allow distributing protocol processing to all of the CPUs except for CPU0 and CPU8.
In this paper, we will refer this setting as {\tt rps = on}.
%
On the other hand, the {\tt rps = off} means that no CPU is allowed for rps. 
Here, the IP protocol processing is performed on the CPUs the initial hardware interrupt is received.
It is configured as the following settings:

\begin{center}
\begin{minipage}{0.8\columnwidth}
\begin{Verbatim}[commandchars=\\\{\}]

\underline{\textbf{rps=off}}
echo 0 > /sys/class/net/eth0/queues/rx-0/rps_cpus
echo 0 > /sys/class/net/eth0/queues/rx-1/rps_cpus
echo 0 > /sys/class/net/eth0/queues/rx-2/rps_cpus
echo 0 > /sys/class/net/eth0/queues/rx-3/rps_cpus

\end{Verbatim}
\end{minipage}
\end{center}

The rps is especially effective when the NIC does not have multiple receive queues or when the number of the queues is 
much smaller than the number of CPU cores. 
That was the case of our experiment, where we had a NIC with only four rx-queues, 
while there was a CPU with eight physical cores.

\section{Result and Discussion}\label{Result and Discussion}

\begin{figure}
\includegraphics[width=\columnwidth]{Figs/ipvs_3figs}
\caption{ipvs results}
\label{fig:ipvs3figs}
\end{figure}

\begin{figure}
\includegraphics[width=\columnwidth]{Figs/iptables_3figs}
\caption{iptables results}
\label{fig:iptabls3figs}
\end{figure}

\begin{figure}
\includegraphics[width=\columnwidth]{Figs/ipvs-iptables-nginx_2figs}
\caption{ipvs and iptables comparison}
\label{fig:ipvs-iptables-nginx_2figs}
\end{figure}


Figure~\ref{fig:ipvs3figs} shows the performance of the ipvs load balancer, that is, 
the throughput (Request/sec) of the nginx web server pods in our experiments.
As for the overlay network, we measured the performance for three backend modes of the flannel, 
host-gw(Figure~\ref{fig:ipvs3figs}(a)), vxlan(Figure~\ref{fig:ipvs3figs}(b)) and udp(Figure~\ref{fig:ipvs3figs}(c)).
%
We compared the performances with the following settings for rss and rps: 

\begin{center}
\begin{minipage}{0.8\columnwidth}
\begin{verbatim}
(rss, rps) = (off, off)
           = (on , off)
           = (off, on )
\end{verbatim}
\end{minipage}
\end{center}

Except for the udp cases, we can see the trend that the throughput linearly increases 
as the number of pods increases and then it eventually saturates.
The performance saturation indicates the maximum performance of the ipvs load balancer.
What limits the maximum performance depends on the flannel backend modes and on the (rss, rps) settings.
From the results in Figures~\ref{fig:ipvs3figs}(a,b), if we turn off distributed packet processing,
{\it i.e.} when \enquote{(rss, rps) = (off, off)}, the performance significantly degrades. 
In this case the bottleneck of the performance is mainly due to packet processing in a single core.

If we compare the results for the cases when \enquote{(rss, rps) = (on, off)} and \enquote{(rss, rps) = (off, on)},
the latter is better than the former.
It is understandable, since in the case of \enquote{rps = on}, eight physical cores are used whereas 
in the case of \enquote{rss = on} only four cores are used on the hardware used in our experiment.
The performance bottleneck of the case when \enquote{rss = on} is considered 
to be due to the fact that the packet processing is done on only four CPU cores.
Now we are investigating the reason for the performance saturation for the case when \enquote{rps = on}, 
and we leave the further investigation for future work.

If we compare the performances among the flannel backend modes, 
the host-gw mode where no encapsulation is conducted shows the highest performance,
followed by the vxlan mode where the Linux kernel encapsulate the Ethernet frame.
The udp mode where flanneld itself encapsulate the IP packet shows the significantly lower performances.

Figure~\ref{fig:iptabls3figs} shows the performance of the load balancer 
functionality of the iptables DNAT. 
The performance value increases as the number of the pod increases linearly, 
then at some point comes to the saturation, as we see in the case with the ipvs results.

If we compare the results for different packet handling settings, the highest performance is 
obtained for the case when \enquote{(rss, rps) = (off on)}, followed by the case when \enquote{(rss, rps) = (on, off)}. 
The performance result for the case when \enquote{(rss, rps) = (off, off)} resulted in the 
poorest performance as the case for the ipvs.
As for the flannel backend modes, the host-gw shows the highest performance followed 
bye the vxlan. The udp backend mode totally degrades the performance.

Figure~\ref{fig:ipvs-iptables-nginx_2figs} compares the performance measurement results 
among three load balancer types, namely, ipvs, iptables DNAT and nginx 
with the condition of \enquote{(rss, rps) = (off on)}.
The proposed ipvs load balancer exhibits almost equivalent performance as the existing iptables 
DNAT based load balancing functionality. 
The nginx based load balancer significantly degrades the performance compared with two other methods.
For the nginx load balancer the performance never increases even though the number of the 
nginx web servers pod is increased.
This is understandable because the nginx as the web server in our experiment only 
returns it's IP address, which is nearly the lightest workload.
The performance of the nginx as a load balancer can not be much better than that of nginx as a web server 
with lightest workload.
Therefor we can expect that the nginx as a load balancer limit the performance to it's own maximum 
even though the web server pods are added to the cluster.

\section{Conclusions}\label{Conclusions}

We proposed a portable load balancer for the Kubernetes cluster system, 
which aims to enable migration of a container cluster for WEB services.
In order to discuss the feasibility of the proposed load balancer, we built 
The Kubernetes cluster system, and conducted performance measurement.
The experimental result indicates that the proposed ipvs based load balancer improves the portability of 
the Kubernetes cluster system while it shows the similar performance as the existing iptables DNAT based load balancer functionality.
We also learned that only the vxlan and the udp backend operation modes could be used 
in the cloud environment and that the udp backend significantly degraded ther performance.
Furthermore, we also learned that the distribution of packet processing among multiple CPUs was very important
to obtain the maximum performance from load balancers.

%\end{document}  % This is where a 'short' article might terminate


\begin{acks}
  The authors would like to thank lots of people.

\end{acks}
